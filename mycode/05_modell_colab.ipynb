{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11PiH-rYzmj4Lh9lZJUSl0djlRbM-RjE2","timestamp":1668693500962}],"collapsed_sections":["0WB0WoF94i4R"],"toc_visible":true,"mount_file_id":"1Lv-UEf3zMmOhzFW8zOKFE_67Nc0KCdSv","authorship_tag":"ABX9TyN2DyGLeNNHB9MmbIYOmq4o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install and Import\n","Wordcloud und Transformer laufen nicht mehr im gleichen Environment"],"metadata":{"id":"-joGsuC54X_E"}},{"cell_type":"code","source":["!pip install Pillow==9.0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"lNxmPgOTJ8ge","executionInfo":{"status":"ok","timestamp":1671658041753,"user_tz":-60,"elapsed":7272,"user":{"displayName":"Marvin Langer","userId":"10460137602450057122"}},"outputId":"ce343481-9147-4d9c-e200-4f9813e57dde"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Pillow==9.0.0\n","  Downloading Pillow-9.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 30.4 MB/s \n","\u001b[?25hInstalling collected packages: Pillow\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","Successfully installed Pillow-9.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{}}]},{"cell_type":"code","source":["!pip install reportlab\n","!python -m spacy download de_core_news_sm\n","!pip install nltk\n","!python -m spacy download de_core_news_md\n","!pip install pyspellchecker\n","!pip install datasets\n","!pip install transformers\n","!pip install fonttools"],"metadata":{"id":"mJ-12j-yzGHa","executionInfo":{"status":"ok","timestamp":1671658127655,"user_tz":-60,"elapsed":85908,"user":{"displayName":"Marvin Langer","userId":"10460137602450057122"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8bb296d5-6a40-44e2-e1c3-c0e3f3b75cac"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting reportlab\n","  Downloading reportlab-3.6.12-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 15.5 MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.8/dist-packages (from reportlab) (9.0.0)\n","Installing collected packages: reportlab\n","Successfully installed reportlab-3.6.12\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting de-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n","\u001b[K     |████████████████████████████████| 14.6 MB 33.3 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from de-core-news-sm==3.4.0) (3.4.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.7)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.5)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (6.3.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.10.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.4.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.12.7)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting de-core-news-md==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-3.4.0/de_core_news_md-3.4.0-py3-none-any.whl (44.4 MB)\n","\u001b[K     |████████████████████████████████| 44.4 MB 454 kB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from de-core-news-md==3.4.0) (3.4.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (1.21.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (8.1.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2.0.7)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (1.0.4)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (3.3.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (1.10.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2.11.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2.4.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (1.0.9)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (0.10.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (21.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (0.10.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (3.0.10)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (0.7.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (4.64.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (57.4.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2.0.8)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (3.0.9)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (4.4.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-md==3.4.0) (2.0.1)\n","Installing collected packages: de-core-news-md\n","Successfully installed de-core-news-md-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_md')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspellchecker\n","  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 32.0 MB/s \n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.7.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n","\u001b[K     |████████████████████████████████| 452 kB 32.4 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 83.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 77.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 80.7 MB/s \n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 70.4 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.8.0 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 30.0 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 64.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.13.2 transformers-4.25.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fonttools\n","  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n","\u001b[K     |████████████████████████████████| 965 kB 24.4 MB/s \n","\u001b[?25hInstalling collected packages: fonttools\n","Successfully installed fonttools-4.38.0\n"]}]},{"cell_type":"code","source":["# Data\n","import pandas as pd\n","import numpy as np\n","import sqlite3\n","from sklearn.model_selection import train_test_split\n","import datasets\n","from datasets import Dataset\n","\n","# NLP\n","import regex as re\n","import spacy\n","from spellchecker import SpellChecker\n","from bs4 import SoupStrainer, BeautifulSoup\n","from bs4.element import Comment\n","\n","# Modelling\n","from transformers import pipeline\n","import os\n","import torch\n","from torch import nn\n","from transformers import pipeline\n","from transformers import AutoTokenizer, BertTokenizer, DistilBertTokenizer\n","from transformers import AutoModelForSequenceClassification, RobertaForSequenceClassification\n","from transformers import BertForSequenceClassification\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers import EarlyStoppingCallback\n","from tqdm import tqdm\n","\n","# Evaluation\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import fontTools\n","import fontTools.subset"],"metadata":{"id":"PyXuxiZuy8tL","executionInfo":{"status":"ok","timestamp":1671658138922,"user_tz":-60,"elapsed":11273,"user":{"displayName":"Marvin Langer","userId":"10460137602450057122"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["tqdm.pandas()\n","pd.options.mode.chained_assignment = None"],"metadata":{"id":"7hFuTBjnSnaz","executionInfo":{"status":"ok","timestamp":1671658138923,"user_tz":-60,"elapsed":23,"user":{"displayName":"Marvin Langer","userId":"10460137602450057122"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# get traceback\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"NLecp94aVO2U","executionInfo":{"status":"ok","timestamp":1671658138923,"user_tz":-60,"elapsed":22,"user":{"displayName":"Marvin Langer","userId":"10460137602450057122"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"qYyY76-Zychs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671658138924,"user_tz":-60,"elapsed":23,"user":{"displayName":"Marvin Langer","userId":"10460137602450057122"}},"outputId":"fd2d2e83-8d00-433e-facc-a298a7822e21"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"JSXt2o4a0Kd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"RwTfBdWVOgW0"}},{"cell_type":"code","source":["DB_CONNECT = \"/content/drive/MyDrive/hareg_nlp/03_Data/hareg.db\"\n","engine = sqlite3.connect(DB_CONNECT)\n","sql = '''SELECT* FROM df_firms'''\n","df = pd.read_sql(sql, engine)\n","df.head()"],"metadata":{"id":"YJI1wAmqzSmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Label"],"metadata":{"id":"h-gbKW8ztLgI"}},{"cell_type":"code","source":["# dictionary mit label und integer als key\n","di_int_to_label = {0: 'Telecommunications',\n","                   1: 'Legal Services',\n","                   2: 'Management Consulting',\n","                   3: 'Medical Practice',\n","                   4: 'Consumer Goods',\n","                   5: 'Leisure, Travel & Tourism',\n","                   6: 'Recreational Facilities and Services',\n","                   7: 'Insurance',\n","                   8: 'Financial Services',\n","                   9: 'Real Estate',\n","                   10: 'Construction',\n","                   11: 'Automotive',\n","                   12: 'Marketing and Advertising',\n","                   13: 'Information Technology and Services',\n","                   14: 'Logistics and Supply Chain',\n","                   15: 'Wholesale',\n","                   16: 'Mechanical or Industrial Engineering',\n","                   17: 'Human Resources',\n","                   18: 'Renewables & Environment'}"],"metadata":{"id":"14xLpvhE1SvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary mit labelkeys als value und label als key\n","di_label_to_int = {'Telecommunications': 0,\n","                   'Legal Services': 1,\n","                   'Management Consulting': 2,\n","                   'Medical Practice': 3,\n","                   'Consumer Goods': 4,\n","                   'Leisure, Travel & Tourism': 5,\n","                   'Recreational Facilities and Services': 6,\n","                   'Insurance': 7,\n","                   'Financial Services': 8,\n","                   'Real Estate': 9,\n","                   'Construction': 10,\n","                   'Automotive': 11,\n","                   'Marketing and Advertising': 12,\n","                   'Information Technology and Services': 13,\n","                   'Logistics and Supply Chain': 14,\n","                   'Wholesale': 15,\n","                   'Mechanical or Industrial Engineering': 16,\n","                   'Human Resources': 17,\n","                   'Renewables & Environment': 18}"],"metadata":{"id":"TrgD3DKgYR7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# change = di_int_to_label\n","change = di_label_to_int\n","df['label_int'] = df['label'].replace(change)"],"metadata":{"id":"VrPvaclGuaTX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cleaning"],"metadata":{"id":"1ddnar5NOxW9"}},{"cell_type":"code","source":["df['gegenstand_raw'] = df['gegenstand']"],"metadata":{"id":"u49GD4sebdLV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# verbleibende Sonderzeichen ersetzen\n","def clean_txt(text):\n","  text = re.sub(\"'\", \"\",text)\n","  text = re.sub(\"(\\W)+\",\" \",text)\n","  return text"],"metadata":{"id":"SI9HWxVFteCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Anzahl Wörter vor Verarbeitung\n","# Die maximal zulässige Inputsequenzlänge von 512 Tokens für BERT Modelle bei Klassifikation-Tasks wird nicht überschritten \n","df['word_count'] = df['gegenstand'].str.split().apply(len)\n","df.word_count.describe()"],"metadata":{"id":"unAblqo8RMrM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['gegenstand'] = df.gegenstand.apply(clean_txt)\n","df['label'] = df.label.replace(di_label_to_int)\n","df[\"tokens\"] = \"\""],"metadata":{"id":"Jr-z590BTW7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n","train, valid = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)"],"metadata":{"id":"GUPhQi7Xs08V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds_train = Dataset.from_pandas(train)\n","ds_test = Dataset.from_pandas(test)\n","ds_valid = Dataset.from_pandas(valid)\n","di_data = datasets.DatasetDict({'train': ds_train, 'test': ds_test, 'valid': ds_valid})"],"metadata":{"id":"5G8XWXl9wAIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spacy\n","\n","Spacy StopWord Removal und (Lemmatization) trägt trotz Transformer Modell deutlich zur Verbesserung der Accuracy und F1 Scores bei."],"metadata":{"id":"tY2Mf9VTn1Gr"}},{"cell_type":"code","source":["# code 1:1 aus pds\n","nlp = spacy.load(\"de_core_news_sm\", exclude=['tok2vec', 'tagger', 'morphologizer', 'parser', 'attribute_ruler', 'ner', 'senter'])\n","\n","def opt_preprocess(text):\n","    remove_list = []\n","\n","    doc = nlp(text.lower())\n","  \n","    non_stop_lem = [token.lemma_ for token in doc if not token.is_stop if not token.is_punct]\n","\n","    for word in non_stop_lem:\n","        if re.findall('[^a-zA-Z0-9_À-ÖØ-öø-ÿ]', word):\n","            remove_list.append(word)\n","\n","    return \" \".join([word for word in non_stop_lem if word not in remove_list])"],"metadata":{"id":"o-_Sn9kln2Ky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['gegenstand'] = df.progress_apply(lambda row: opt_preprocess(row['gegenstand']), axis=1)"],"metadata":{"id":"zdqHvj79pyK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['gegenstand'] = df.gegenstand.apply(lambda text: text.lower())"],"metadata":{"id":"5isEchJ1d3nZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lemmatization (2)\n","Beim betrachten der Daten fällt auf, dass die Lemmatization nicht richtig funktioniert hat. Daher wird diese erneut durchgeführt.\n","Tatsächlich verschlechtert die korrekte Lemmatization die Vorhersagegenauigkeit um ~ 3% Accuracy/F1-Score. Daher wird diese nun doch nicht mehr ausgeführt."],"metadata":{"id":"7FttvccGb6iI"}},{"cell_type":"code","source":["nlp = spacy.load('de_core_news_md')\n","\n","def lemma(text):\n","    doc = nlp(text.lower())\n","    text = \" \".join(w.lemma_.lower() for w in doc) # if not w.is_stop if not w.is_punct)\n","    return text"],"metadata":{"id":"Wqf6SUDYb5j1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df['gegenstand'] = df.progress_apply(lambda row: lemma(row['gegenstand']), axis=1)"],"metadata":{"id":"AE2HlkzFRK7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stop Words (2)\n","\n","Erweiterete Stopword Liste führt erwartungsgemäß zu schlechteren Ergebnisse. Daher nur einzelne Buchstaben entfernen, was mit einem Regex im Nachhinein betrachtet wohl einfacher gewesen wäre. Doch das entfernen der Buchstaben führt ebenfalls zu signifikanten Einbußen. Die einzelnen Buchstaben sind höchstwahrscheinlich Gliederungspunkte. Der Transformer kann mit Hilfe dieser Punkte anscheinend den Zusammenhang der einzelnen Absätze besser konstruieren. Daher werden diese nun doch nicht entfernt. Wiederum verbessert die Entfernung einzelner Buchstaben aus dem Webseiten Text die Klassifikation."],"metadata":{"id":"wh-1XDJojuuA"}},{"cell_type":"code","source":["# stopword liste von https://countwordsfree.com/stopwords/german\n","stopword_file = open(\"/content/drive/MyDrive/hareg_nlp/03_Data/stop_words_german.txt\", \"r\")\n","stop_words_data = stopword_file.read()  \n","customize_stop_words = stop_words_data.replace('\\n', ' ').split(\" \")\n","stopword_file.close()\n","print(customize_stop_words)"],"metadata":{"id":"9Kps5iKvQ0oz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["li_letter = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"],"metadata":{"id":"fTHW3FlXNzFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(nlp.Defaults.stop_words)"],"metadata":{"id":"eFIpCykWKlWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load('de_core_news_md')"],"metadata":{"id":"oiioD0m-LdUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_stop_words(text):\n","\n","    # add custom stop words\n","    for word in customize_stop_words:\n","        lex = nlp.vocab[word]\n","        lex.is_stop = True\n","\n","    lst=[]\n","    stopwords = nlp.Defaults.stop_words\n","\n","    for w in text.split():\n","        if w.lower() not in stopwords:    #checking whether the word is not \n","            lst.append(w)                    #present in the stopword list.\n","\n","    new_text = ' '.join(lst)        \n","    return new_text"],"metadata":{"id":"-4gNaU7Ojwn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_letter(text):\n","    \n","    # add custom stop words\n","    for word in li_letter:\n","        lex = nlp.vocab[word]\n","        lex.is_stop = True\n","\n","    lst=[]\n","    stopwords = nlp.Defaults.stop_words\n","\n","    for w in text.split():\n","        if w.lower() not in stopwords:    #checking whether the word is not \n","            lst.append(w)                    #present in the stopword list.\n","\n","    new_text = ' '.join(lst)        \n","    return new_text "],"metadata":{"id":"jt7R5H7aV7XG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df['gegenstand_wordcloud'] = df.progress_apply(lambda row: remove_stop_words(row['gegenstand']), axis=1)"],"metadata":{"id":"hDvB5iPKjwqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spellchecker"],"metadata":{"id":"x-fWkQel90Vp"}},{"cell_type":"code","source":["spell = SpellChecker(language='de')\n","\n","def spellcheck(text):\n","    text = text.split()\n","    # find those words that may be misspelled\n","    misspelled = spell.unknown(text)\n","\n","    for word in misspelled:\n","        correction = spell.correction(word)\n","        # print(correction)\n","        text = [(w.replace(w, correction) if ((w == word) & (correction is not None)) else w) for w in text]\n","    \n","    print(text)\n","    text = \" \".join(text)\n","    return text"],"metadata":{"id":"ItqmDMQP91xx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"hier ist was fual udn es stimmt nohc mehr nihct\"\n","spellcheck(text)"],"metadata":{"id":"YFQcV2sY-xLT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Der Unternehmensgegenstand ist nahezu fehlerfrei. Daher neigt der Spellchecker zur Verschlimmbesserung und vermindert dadurch Accuracy und F1-Score."],"metadata":{"id":"_Go42tNDRN3Q"}},{"cell_type":"code","source":["# df['gegenstand'] = df.progress_apply(lambda row: spellcheck(row['gegenstand']), axis=1)"],"metadata":{"id":"hDqrpt_fBKB-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing Ideas"],"metadata":{"id":"3v3CTAIQ-ymS"}},{"cell_type":"code","source":["# noch nicht umgesetzte Ideen:\n","# zusammengesetzte nomen aufsplitten und sowohl die komponenten als auch das urspgl. nomen behalten\n","# n/bigrams\n","# enhance sentences\n","# einzelne englishe wörter nach deutsch Übersetzen\n","\n","# für html - umlaute welche in html durch code dargestellt werden vor stop word removal wiederherstellen\n","# sieh dazu auch https://stackoverflow.com/questions/46613734/how-do-i-replace-xc3-etc-with-umlauts\n","# spellchekcer nutzen, um aus Versehen getrennte Worte nach utf-8 umlaut wieder zusammenzuführen"],"metadata":{"id":"qt4cnUQt6k7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DB_CONNECT = '/content/drive/MyDrive/hareg_nlp/03_Data/hareg.db'\n","engine = sqlite3.connect(DB_CONNECT)\n","### df.to_sql('df_spellcheck', con=engine, if_exists='fail', index = False, chunksize=10000)"],"metadata":{"id":"eIPfeNRzPbdn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenization\n","try to train custom tokenizer \n","https://www.youtube.com/watch?v=DJimQynXZsQ\n","https://www.youtube.com/watch?v=MR8tZm5ViWU&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o&index=55"],"metadata":{"id":"CXnPPQETg_rL"}},{"cell_type":"code","source":["# distilbert-base-uncased\n","# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"],"metadata":{"id":"P_Jf4sE9dAVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df['tokens'] = df['gegenstand'].apply(lambda row: tokenizer(row, padding='max_length', truncation=True))\n","def make_tokens(data):\n","    return tokenizer(data['text'], padding='max_length', truncation=True)"],"metadata":{"id":"ZFuyvUYRwZxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["di_data = di_data.map(make_tokens, batched=True)"],"metadata":{"id":"4gVJJgDNiR0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds_train = di_data[\"train\"]\n","ds_test = di_data[\"test\"]\n","ds_valid = di_data[\"valid\"]"],"metadata":{"id":"F1dHUq0e33w0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Class Weights"],"metadata":{"id":"a6QCmmf-o_hA"}},{"cell_type":"code","source":["class_weights = (1- (train['label'].value_counts().sort_index()/len(train))).values\n","class_weights = torch.from_numpy(class_weights).float().to('cuda')\n","class_weights"],"metadata":{"id":"f8CTgsFOo3Gs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# new distil-BERT\n","\n","Vor der Klassifikation könnte zudem ein binäres Klassifikationsmodell eingesetzt werden, um Holding-Gesellschaften herauszufiltern."],"metadata":{"id":"07Xu0tCD9346"}},{"cell_type":"markdown","source":["## Fine-Tune "],"metadata":{"id":"qgbfLE2HOzav"}},{"cell_type":"code","source":["# model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=19)\n","model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=19)"],"metadata":{"id":"RiKzCSHgdc-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(output_dir='trainer',\n","                                  evaluation_strategy='steps',\n","                                  eval_steps = 250,\n","                                  save_total_limit = 5,\n","                                  num_train_epochs=8,\n","                                  per_device_train_batch_size = 16,\n","                                  gradient_accumulation_steps = 2,\n","                                  logging_dir='./logs',\n","                                  logging_steps=1,\n","                                  metric_for_best_model = 'f1',\n","                                  load_best_model_at_end=True)"],"metadata":{"id":"CdnXk-XlhgIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trainer class from https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.Trainer\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        # compute custom loss (suppose one has 3 labels with different weights)\n","        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"],"metadata":{"id":"z4KX3q4bhnJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"metadata":{"id":"ocD6ZH_gh2nY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = CustomTrainer(model = model,\n","                        args = training_args, \n","                        train_dataset = ds_train, \n","                        eval_dataset = ds_valid, \n","                        compute_metrics = compute_metrics,\n","                        tokenizer = tokenizer,\n","                        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])"],"metadata":{"id":"p8m6SQ0jh3t1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"Di78cuwah5r2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(\"/content/drive/MyDrive/hareg_nlp/04_Models/dist_bert\")"],"metadata":{"id":"0BhvlRfeskkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenized input\n","ds_train[0]"],"metadata":{"id":"j5UYnp-f0Xke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show hidden embedding\n","model.bert.embeddings.word_embeddings.weight"],"metadata":{"id":"L1mxmtJ0CYBN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predict"],"metadata":{"id":"bOOpojEm98Lh"}},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/hareg_nlp/04_Models/dist_bert\"\n","\n","pipe = pipeline('text-classification',\n","    model=path,\n","    device=0,\n","    truncation=True)"],"metadata":{"id":"05KKbVtXB-Eu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test['label_pred'] = test.progress_apply(lambda row: pipe(row['gegenstand'])[0]['label'].replace('LABEL_',''), axis=1)\n","test.label_pred = test.label_pred.astype(int)"],"metadata":{"id":"H0-TvtlB99Kf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"uiuOQFczO1mM"}},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"id":"AWyR6nv68QVv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = test['label']\n","y_pred = test['label_pred']\n","\n","accuracy = balanced_accuracy_score(y_true, y_pred)\n","f1_metric = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","print(\"Balanced Accuracy: {:.4f}\".format(accuracy))\n","print(\"F1-score: {:.4f}\".format(f1_metric))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))"],"metadata":{"id":"NrN1wlb69Ps_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# new ger-BERT"],"metadata":{"id":"EGTVb5z_qci1"}},{"cell_type":"markdown","source":["## Fine-Tune"],"metadata":{"id":"2rdSnI6g4ih5"}},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-german-cased', num_labels=19)"],"metadata":{"id":"2h2roys65B9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = CustomTrainer(model = model,\n","                        args = training_args, \n","                        train_dataset = ds_train, \n","                        eval_dataset = ds_valid, \n","                        compute_metrics = compute_metrics,\n","                        tokenizer = tokenizer,\n","                        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])"],"metadata":{"id":"YJYcmpFH5wuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"Yb9ttTYF5mNv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(\"/content/drive/MyDrive/hareg_nlp/04_Models/ger_bert\")"],"metadata":{"id":"GjM7D0DH5mQd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predict"],"metadata":{"id":"0WB0WoF94i4R"}},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/hareg_nlp/04_Models/ger_bert\"\n","\n","pipe = pipeline('text-classification',\n","    model=path,\n","    device=0,\n","    truncation=True)"],"metadata":{"id":"90KUdhRV5CYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test['label_pred'] = test.progress_apply(lambda row: pipe(row['gegenstand'])[0]['label'].replace('LABEL_',''), axis=1)\n","test.label_pred = test.label_pred.astype(int)"],"metadata":{"id":"cwZ-wkYM5CbB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"xjKoYlHd4i6U"}},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"id":"jry-FqKQ8L6j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = test['label']\n","y_pred = test['label_pred']\n","\n","accuracy = balanced_accuracy_score(y_true, y_pred)\n","f1_metric = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","print(\"Accuracy: {:.4f}\".format(accuracy))\n","print(\"F1-score: {:.4f}\".format(f1_metric))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))"],"metadata":{"id":"KsHUeJLJ5C6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# old distil-BERT"],"metadata":{"id":"OMOB_6FcOjNt"}},{"cell_type":"code","source":["path_old_model = \"/content/drive/MyDrive/capstoneproject/models/transformer/distilbert_overlap_chunking_checkpoint-2500\""],"metadata":{"id":"repF7Nd1lrKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Single Predictions"],"metadata":{"id":"QgVI0e4P_9QX"}},{"cell_type":"code","source":["def old_predict(txt_input):\n","    pipe = pipeline('text-classification',\n","                    model=path_old_model,\n","                    device=0,\n","                    truncation=True\n","                    )\n","    model_output = pipe(txt_input)\n","    label_id = int(model_output[0]['label'].replace('LABEL_',''))\n","    score = model_output[0]['score']*100\n","    score = float(\"{:.2f}\".format(score))\n","    label = di_int_to_label[label_id]\n","    \n","    return label, score"],"metadata":{"id":"oP-AxKqP__GO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try_input = df.loc[100, \"gegenstand\"]\n","print(old_predict(try_input),\"\\n\",try_input)"],"metadata":{"id":"uLnBML_XALQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_old = pipeline('text-classification',\n","           model=path_old_model,\n","           device=0,\n","           truncation=True)"],"metadata":{"id":"5ZSFN-rXvLjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test Sample"],"metadata":{"id":"nydgV3Hb1W7Z"}},{"cell_type":"code","source":["test['label_pred'] = \"\"\n","test['label_pred'] = test.progress_apply(lambda row: pipe_old(row['gegenstand'])[0]['label'].replace('LABEL_',''), axis=1)\n","test.label_pred = test.label_pred.astype(int)"],"metadata":{"id":"RkhDy7hcvLly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = test['label']\n","y_pred = test['label_pred']\n","\n","accuracy = balanced_accuracy_score(y_true, y_pred)\n","f1_metric = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","print(\"Accuracy: {:.4f}\".format(accuracy))\n","print(\"F1-score: {:.4f}\".format(f1_metric))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))"],"metadata":{"id":"xG4QsBZfvLoO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Whole Sample"],"metadata":{"id":"7yR9s3Ce1bKx"}},{"cell_type":"code","source":["df_all = df"],"metadata":{"id":"1naJpaaEJkn8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_all['label_pred'] = \"\"\n","df_all['label_pred'] = df_all.progress_apply(lambda row: pipe_old(row['gegenstand'])[0]['label'].replace('LABEL_',''), axis=1)\n","df_all.label_pred = df_all.label_pred.astype(int)"],"metadata":{"id":"iQYOo4u8xBKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = df_all['label']\n","y_pred = df_all['label_pred']\n","\n","# average = weighted -> impliziert accuracy = recall\n","accuracy = balanced_accuracy_score(y_true, y_pred)\n","f1_metric = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","print(\"Accuracy: {:.4f}\".format(accuracy))\n","print(\"F1-score: {:.4f}\".format(f1_metric))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))"],"metadata":{"id":"dV5mKxJTxBM0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Classification Report"],"metadata":{"id":"JO0VjakQAxCO"}},{"cell_type":"code","source":["df_all['label_pred_txt'] = df_all['label_pred'].replace(di_int_to_label)\n","\n","change = di_int_to_label\n","# change = di_label_to_int\n","df_all['label_pred'] = df_all['label_pred'].replace(change)\n","df_all['label'] = df_all['label'].replace(change)"],"metadata":{"id":"2JD8UFSgA1HW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(df_all['label'].to_list(), df_all['label_pred'].to_list()))"],"metadata":{"id":"dF3YZUlJA9C-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Confusion Matrix"],"metadata":{"id":"A2Y25c53AE1V"}},{"cell_type":"code","source":["labels = list(di_int_to_label.values())\n","fig, ax = plt.subplots(figsize=(12,12)) \n","cm = confusion_matrix(df_all['label'], df_all['label_pred'] )\n","f = sns.heatmap(cm, annot=True, fmt='d', linewidths=0.0, xticklabels=labels, yticklabels=labels)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.savefig('cm_whole.svg', bbox_inches='tight',\n","            dpi=300)\n"],"metadata":{"id":"9_9CAdje_5nP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = list(di_int_to_label.values())\n","cm = confusion_matrix(df_all['label'], df_all['label_pred'])\n","cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100"],"metadata":{"id":"vHyHYhN86CxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(12,12))\n","\n","f = sns.heatmap(cmn, annot=True, fmt='.0f', xticklabels=labels, yticklabels=labels)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.savefig('cm_whole_relative.svg', bbox_inches='tight')"],"metadata":{"id":"eA1_A6wgfly4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Whole Sample Only Long\n","Durch die EDA wissen wir, dass die durchschnittliche Anzahl Zeichen ~520 beträgt. Der Median liegt bei ~344. Es existieren also viele Texte die kürzer als der Durchnittswert sind, aber auch einige lange Texte, welche die Verteilung positiv verzerren (positively skewed).\n","Wir filtern willkürlich bei einem Wert leicht unterhalb des Medians und beobachten, dass sowohl Accuracy als auch F1-Score um gut 7 bis 8 Prozentpunkte ansteigen.\n"],"metadata":{"id":"lxpgXqdaJbAj"}},{"cell_type":"code","source":["df_long = df.loc[(df_all[\"zeichen\"] >= 300)]\n","df_long.describe()"],"metadata":{"id":"jfsNK-7IPXZ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_long['label_pred'] = \"\"\n","df_long['label_pred'] = df_long.progress_apply(lambda row: pipe_old(row['gegenstand'])[0]['label'].replace('LABEL_',''), axis=1)\n","df_long.label_pred = df_long.label_pred.astype(int)"],"metadata":{"id":"iR_M9a_BNDKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_long['label_pred_txt'] = df_long['label_pred'].replace(di_int_to_label)\n","\n","# change = di_int_to_label\n","change = di_label_to_int\n","df_long['label_pred'] = df_long['label_pred'].replace(change)\n","df_long['label'] = df_long['label'].replace(change)"],"metadata":{"id":"zmksROHnRNoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = df_long['label']\n","y_pred = df_long['label_pred']\n","\n","accuracy = balanced_accuracy_score(y_true, y_pred)\n","f1_metric = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","print(\"Accuracy: {:.4f}\".format(accuracy))\n","print(\"F1-score: {:.4f}\".format(f1_metric))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))"],"metadata":{"id":"pLHKXmTGQY7Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Class Report"],"metadata":{"id":"dvf23XBdV0HO"}},{"cell_type":"code","source":["df_long['label_pred_txt'] = df_long['label_pred'].replace(di_int_to_label)\n","\n","change = di_int_to_label\n","# change = di_label_to_int\n","df_long['label_pred'] = df_long['label_pred'].replace(change)\n","df_long['label'] = df_long['label'].replace(change)"],"metadata":{"id":"jyxzkHADV2GJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(df_long['label'].to_list(), df_long['label_pred'].to_list()))"],"metadata":{"id":"Rk8C2mNuTtMu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Conf Matrix"],"metadata":{"id":"e8o53zbKV4vR"}},{"cell_type":"code","source":["labels = list(di_int_to_label.values())\n","fig, ax = plt.subplots(figsize=(12,12)) \n","cm = confusion_matrix(df_long['label'], df_long['label_pred'] )\n","f = sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.savefig('cm_whole_long.svg', bbox_inches='tight',\n","            dpi=300)"],"metadata":{"id":"2yVrBAormhoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = list(di_int_to_label.values())\n","cm = confusion_matrix(df_long['label'], df_long['label_pred'])\n","cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100"],"metadata":{"id":"-eTPoYet6Qqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(12,12))\n","f = sns.heatmap(cmn, annot=True, fmt='.0f', xticklabels=labels, yticklabels=labels)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.savefig('cm_whole_long_relative.svg', bbox_inches='tight')"],"metadata":{"id":"8jo7vdRhwzzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Website Classifier"],"metadata":{"id":"fkH27DJWQcJX"}},{"cell_type":"markdown","source":["### Parsen"],"metadata":{"id":"_0dXwD82kVoO"}},{"cell_type":"code","source":["# code zu tag_visible und extract_text aus pds\n","def tag_visible(element):\n","  if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n","      return False\n","  if isinstance(element, Comment):\n","      return False\n","  return True\n","\n","def extract_text(raw_html):\n","  raw_text = ''\n","  description = ''\n","  keywords = ''\n","\n","  try:\n","    soup = BeautifulSoup(raw_html, 'lxml')\n","    #text\n","    text = soup.find_all(text=True)\n","    visible_texts = filter(tag_visible, text)\n","    text = u\"°\".join(t.strip() for t in visible_texts)\n","    raw_text = re.sub(r\"(\\s\\s)+\", \" \", text)\n","  except:\n","    pass\n","    \n","  #meta\n","  try:\n","    description = soup.find(\"meta\", attrs={'name': 'description'})['content']\n","  except:\n","    try:\n","      description = soup.find(\"meta\", property=\"og:description\")['content']\n","    except:\n","      pass\n","  try:\n","    keywords = soup.find(\"meta\", attrs={'name': 'keywords'})['content']\n","  except:\n","    pass\n","\n","\n","  return raw_text, description, keywords"],"metadata":{"id":"k5MVb7IwT2uw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[['site_text', 'description', 'keywords']] = df.progress_apply(lambda row: extract_text(row['website']), axis=1, result_type='expand')"],"metadata":{"id":"DypbCWscjQSn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decode UTF-8\n","Wrd aktuell nicht benötigt, die strings enthalten non ascii letter, daher ist die Dekodierung der Umlaute nicht ohne weiteres umzusetzen"],"metadata":{"id":"4pk15ByRkOAu"}},{"cell_type":"code","source":["# zur besseren Verarbeitung schon mal bestimmte Sonderzeichen entfernen\n","def clean_ascii(text):\n","    text = re.sub('°', ' ', str(text))\n","    text = re.sub(\"b' \", '', str(text))\n","    text = str(text).replace('\\\\\\\\', '\\\\')\n","    text = text.replace(':', '')\n","    return text"],"metadata":{"id":"wX2vn3btlMsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = df.loc[4, \"site_text\"] \n","bs = s.encode('raw-unicode-escape')  # encode to bytes without double-encoding\n","print(bs)"],"metadata":{"id":"5yhZha_yYKWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = 'und m\\xc3\\xb6glicherweise'\n","bs = s.encode('raw-unicode-escape')  # encode to bytes without double-encoding\n","print(bs)\n"],"metadata":{"id":"Zi4l6N88X9vP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoded = bs.decode('utf-8')\n","print(decoded)"],"metadata":{"id":"pkZV154XkDgS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cleanen"],"metadata":{"id":"3f3Uub0I0v_w"}},{"cell_type":"code","source":["df['full_text'] = df['site_text'] + ' ' + df['description'] + ' ' + df['keywords']\n","df['full_text_raw'] = df['site_text'] + ' ' + df['description'] + ' ' + df['keywords']"],"metadata":{"id":"p2lPKPSFb09k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean(text):\n","    text = text.replace('\\n',' ')\n","    text = text.replace('\\\\n',' ')\n","    text = text.replace('html','')\n","    text = re.sub('°', ' ', str(text))\n","    text = re.sub('  ', ' ', str(text))\n","    text = re.sub(',+', ',', str(text))\n","    text = re.sub(\"'\", \"\", str(text))\n","    text = re.sub(\"\\b(\\w)\\b\", \"\", str(text))\n","    return text"],"metadata":{"id":"pzWTjA9FXcXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_words(text, char_list):\n","    flag = 0\n","    in_list = text.split()\n","    new_list = []\n","    for line in in_list:\n","        new_words = ' '.join([word for word in line.split() if not any([phrase in word for phrase in char_list])])\n","        new_list.append(new_words)\n","        new_text = \" \".join(new_list)\n","        flag = 1\n","    if flag == 1:\n","        return new_text\n","    else:\n","        return text"],"metadata":{"id":"5rW764ZWgZ88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_sonderzeichen(text):\n","    text = re.sub(\"(\\W)+\", \" \", str(text))\n","    return text"],"metadata":{"id":"ct0J7PIcjUWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_special_words(text):\n","    text = text.replace('inkl','')\n","    text = text.replace('mwst','')\n","    return text"],"metadata":{"id":"JDRKfJ2n6129"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# allmögliches\n","df['full_text'] = df['full_text'].apply(lambda row: clean(row))"],"metadata":{"id":"DMRmfS17kzK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove words die bestimmte Sonderzeichen enthalten\n","char_list = [\"\\\\\", \"{\", \"1\", \"2\", \"3\",  \"4\",  \"5\",  \"6\",  \"7\",  \"8\",  \"9\", \"0\"]\n","df['full_text'] = df['full_text'].progress_apply(lambda row: remove_words(row, char_list))"],"metadata":{"id":"h8hKpvfCeL-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove alle speziellen Zeichen und Zahlen\n","df['full_text'] = df['full_text'].apply(lambda row: clean_sonderzeichen(row))"],"metadata":{"id":"x-WLJWYajBPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# angepasstes preprocessing\n","df['full_text'] = df.progress_apply(lambda row: opt_preprocess(row['full_text']), axis=1)"],"metadata":{"id":"cfeiOkKRXitH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tolowercase oder Funktion lemma mit Lemmatizing nutzen\n","# lemmatizing verbessert F1-Score um 0,01 Prozentpunkte\n","df['full_text'] = df.full_text.apply(lambda text: text.lower())\n","df['full_text'] = df.full_text.progress_apply(lambda text: lemma(text)) "],"metadata":{"id":"zjBgIGeosJCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# einzelne letter entfernen\n","df['full_text'] = df.progress_apply(lambda row: remove_letter(row['full_text']), axis=1)"],"metadata":{"id":"MOX9uajwWvh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# einzelne letter entfernen (2), weil nach (1) immernoch einzelne letter enthalten sind\n","def kill_single_letter(text):\n","\n","    new_text = ' '.join( [w for w in text.split() if len(w)>1] )\n","\n","    return new_text\n","\n","df['full_text'] = df.progress_apply(lambda row: kill_single_letter(row['full_text']), axis=1)"],"metadata":{"id":"bIcuGC1-ZV3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# final strip\n","df['full_text'] = df.full_text.progress_apply(lambda text: re.sub(' +', ' ', text)) "],"metadata":{"id":"ojgGHCgA6Dqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove special words\n","# df['full_text'] = df.full_text.progress_apply(lambda text: remove_special_words(text)) \n","# !besser nicht, haben Aussagekraft"],"metadata":{"id":"MhQ6Fq_26_8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Classify Website"],"metadata":{"id":"tsfSPbpmtU_O"}},{"cell_type":"code","source":["df['web_pred'] = df.progress_apply(lambda row: pipe_old(row['full_text'])[0]['label'].replace('LABEL_',''), axis=1)\n","df['web_pred'] = df['web_pred'].astype(int)"],"metadata":{"id":"b-CoWowqtbgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = df['label_int']\n","y_pred = df['web_pred']\n","\n","accuracy = balanced_accuracy_score(y_true, y_pred)\n","f1_metric = f1_score(y_true, y_pred, average=\"weighted\")\n","precision = precision_score(y_true, y_pred, average=\"weighted\")\n","recall = recall_score(y_true, y_pred, average=\"weighted\")\n","\n","print(\"Accuracy: {:.4f}\".format(accuracy))\n","print(\"F1-score: {:.4f}\".format(f1_metric))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))"],"metadata":{"id":"zdMIhaIotblM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['web_pred_txt'] = df['web_pred'].replace(di_int_to_label)\n","print(classification_report(df['label'].to_list(), df['web_pred_txt'].to_list()))"],"metadata":{"id":"0NmTNXtR0uKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = list(di_int_to_label.values())\n","fig, ax = plt.subplots(figsize=(12,12)) \n","cm = confusion_matrix(df_all['label'], df_all['web_pred_txt'] )\n","f = sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.savefig('cm_website.svg', bbox_inches='tight',\n","            dpi=300)"],"metadata":{"id":"NR2bDKZq1P6l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Classify Combi"],"metadata":{"id":"pUpIA2EbtYe1"}},{"cell_type":"code","source":["df['combi_text'] = df['gegenstand'] + ' ' + df['full_text'] "],"metadata":{"id":"Vr18EfYItayF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['combi_pred'] = df.progress_apply(lambda row: pipe_old(row['combi_text'])[0]['label'].replace('LABEL_',''), axis=1)\n","df['combi_pred'] = df['combi_pred'].astype(int)"],"metadata":{"id":"weFUZ1WEwKD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = df['label_int']\n","y_pred = df['combi_pred']\n","\n","accuracy = balanced_accuracy_score(y_true, y_pred)\n","f1_metric = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","print(\"Accuracy: {:.4f}\".format(accuracy))\n","print(\"F1-score: {:.4f}\".format(f1_metric))\n","print(\"Precision: {:.4f}\".format(precision))\n","print(\"Recall: {:.4f}\".format(recall))"],"metadata":{"id":"GO21brrTwKGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['combi_pred_txt'] = df['combi_pred'].replace(di_int_to_label)\n","print(classification_report(df['label'].to_list(), df['combi_pred_txt'].to_list()))"],"metadata":{"id":"gmKr2iua1kIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = list(di_int_to_label.values())\n","fig, ax = plt.subplots(figsize=(12,12)) \n","cm = confusion_matrix(df_all['label'], df_all['combi_pred_txt'] )\n","f = sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n","plt.ylabel('Actual')\n","plt.xlabel('Predicted')\n","plt.savefig('cm_combi.svg', bbox_inches='tight',\n","            dpi=300)"],"metadata":{"id":"H1Odv17L24g-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Plot Precision-Recall curve for each class and iso-f1 curves\n","# # von https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n","\n","# import matplotlib.pyplot as plt\n","# from itertools import cycle\n","\n","# # setup plot details\n","# colors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n","\n","# _, ax = plt.subplots(figsize=(7, 8))\n","\n","# f_scores = np.linspace(0.2, 0.8, num=4)\n","# lines, labels = [], []\n","# for f_score in f_scores:\n","#     x = np.linspace(0.01, 1)\n","#     y = f_score * x / (2 * x - f_score)\n","#     (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n","#     plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n","\n","# display = PrecisionRecallDisplay(\n","#     recall=recall[\"micro\"],\n","#     precision=precision[\"micro\"],\n","#     average_precision=average_precision[\"micro\"],\n","# )\n","# display.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n","\n","# for i, color in zip(range(n_classes), colors):\n","#     display = PrecisionRecallDisplay(\n","#         recall=recall[i],\n","#         precision=precision[i],\n","#         average_precision=average_precision[i],\n","#     )\n","#     display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\n","\n","# # add the legend for the iso-f1 curves\n","# handles, labels = display.ax_.get_legend_handles_labels()\n","# handles.extend([l])\n","# labels.extend([\"iso-f1 curves\"])\n","# # set the legend and the axes\n","# ax.set_xlim([0.0, 1.0])\n","# ax.set_ylim([0.0, 1.05])\n","# ax.legend(handles=handles, labels=labels, loc=\"best\")\n","# ax.set_title(\"Extension of Precision-Recall curve to multi-class\")\n","\n","# plt.show()"],"metadata":{"id":"3oBj4el412Z8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Combine seperate Classifiers"],"metadata":{"id":"fnVVmZFwxpV8"}},{"cell_type":"code","source":["# direkt argmax der W'keiten beider Classifier\n","# oder erst Durchschnitt der W'keiten beider Classifier bilden, dann argmax\n","# weitere? was sagt die Literatur?\n","# Trainieren separeter Modelle \n","# wie können mehrer DL Modelle kombiniert werden?"],"metadata":{"id":"D4jcJ3_sxsCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Wordcloud\n","\n","Wordcloud und Transformer laufen nicht mehr im gleichen Environment"],"metadata":{"id":"VqPDyVu4YhMA"}},{"cell_type":"code","source":["!pip install -U pillow"],"metadata":{"id":"42vWptr_Qh4F"},"execution_count":null,"outputs":[]}]}