{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "\n",
    "# waits\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "# driver\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "# selenium\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# other\n",
    "# from ssl import Options\n",
    "\n",
    "# multi\n",
    "from concurrent import futures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver(ip=\"\", port=\"\", flag_proxy=False):\n",
    "    # PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "    chrome_options = uc.ChromeOptions()\n",
    "\n",
    "    # Removes navigator.webdriver flag\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    # prevent bugs and skip popups\n",
    "    chrome_options.add_argument('--no-first-run --no-service-autorun --password-store=basic')\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    # uc - user agent\n",
    "    # bot detection can determine if os in user-agent differs from real os\n",
    "    # chrome_options.add_argument(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.5 Safari/605.1.15\")\n",
    "    # chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\")\n",
    "\n",
    "    # add multiple download flag\n",
    "    dl_multi = {'profile.default_content_setting_values.automatic_downloads': 1}\n",
    "    chrome_options.add_experimental_option(\"prefs\", dl_multi)\n",
    "\n",
    "    # add download location\n",
    "    dl_path = {\"download.default_directory\" : \"C:\\\\Users\\\\lange\\\\OneDrive\\\\Dokumente\\\\Studium\\\\06_Abschlussarbeiten\\\\Seminar\\\\02_Download\"}\n",
    "    chrome_options.add_experimental_option(\"prefs\", dl_path)\n",
    "\n",
    "    # add profile\n",
    "    # uc.Chrome(user_data_dir='C:\\\\Users\\\\lange\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\\\\Default')\n",
    "    \n",
    "    # add proxy if True\n",
    "    # -1 to use first element after use of driver to scrape proxies\n",
    "    if flag_proxy == True:       \n",
    "        PROXY = str(ip + \":\" + port)\n",
    "        print(PROXY)\n",
    "        chrome_options.add_argument('--proxy-server=http://%s' % PROXY)\n",
    "\n",
    "    # # be careful with headless browsing\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    \n",
    "    # # additional options to allow downloading while in headless mode\n",
    "    # prefs = {\n",
    "    # \"download.default_directory\": \"C:\\\\Users\\\\lange\\\\OneDrive\\\\Dokumente\\\\Studium\\\\06_Abschlussarbeiten\\\\Seminar\\\\02_Download\",\n",
    "    # \"download.directory_upgrade\": True,\n",
    "    # \"download.prompt_for_download\": False,\n",
    "    # \"plugins.always_open_pdf_externally\": True,\n",
    "    # \"profile.default_content_settings.popups\": \"0\",\n",
    "    # \"profile.content_settings.exceptions.automatic_downloads.*.setting\": \"1\"\n",
    "    # }\n",
    "    # args = [\"--no-sandbox\",\n",
    "    #         \"--disable-dev-shm-usage\"]\n",
    "    \n",
    "    # chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    # chrome_options.add_experimental_option(\"args\", args)\n",
    "    # # chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    # webdriver vs. undetected chromedriver\n",
    "    # driver = webdriver.Chrome(executable_path=PATH, options=chrome_options)\n",
    "    driver = uc.Chrome(options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keine Verwendung, da public proxies zu 99% nicht funktionieren\n",
    "# chromedriver_autoinstaller.install() # To update your chromedriver automatically\n",
    "# driver = webdriver.Chrome(\"C:\\\\Program Files (x86)\\\\chromedriver\")\n",
    "\n",
    "# Get free proxies for rotating\n",
    "def get_list_proxies(driver):\n",
    "    driver.get('https://sslproxies.org')\n",
    "\n",
    "    table = driver.find_element(By.TAG_NAME, 'table')\n",
    "    thead = table.find_element(By.TAG_NAME, 'thead').find_elements(By.TAG_NAME, 'th')\n",
    "    tbody = table.find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "    headers = []\n",
    "    for th in thead:\n",
    "        headers.append(th.text.strip())\n",
    "\n",
    "    proxies = []\n",
    "    for tr in tbody:\n",
    "        proxy_data = {}\n",
    "        tds = tr.find_elements(By.TAG_NAME, 'td')\n",
    "        for i in range(len(headers)):\n",
    "            proxy_data[headers[i]] = tds[i].text.strip()\n",
    "        proxies.append(proxy_data)\n",
    "    \n",
    "    driver.quit()\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dlb_elements(driver):\n",
    "    # try to extract clickable elements, if not possible, because their is no link, set empty\n",
    "    try:\n",
    "        AD_elements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.LINK_TEXT, \"AD\")))\n",
    "    except:\n",
    "        AD_elements = []\n",
    "    try:\n",
    "        CD_elements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.LINK_TEXT, \"CD\")))\n",
    "    except:\n",
    "        CD_elements = []\n",
    "    try:\n",
    "        HD_elements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.LINK_TEXT, \"HD\")))\n",
    "    except:\n",
    "        HD_elements = []\n",
    "    \n",
    "    # create dict of elements from extracted lists\n",
    "    dict_dlb_elements = {}\n",
    "    dict_dlb_elements[\"AD\"] = AD_elements\n",
    "    dict_dlb_elements[\"CD\"] = CD_elements\n",
    "    dict_dlb_elements[\"HD\"] = HD_elements\n",
    "    \n",
    "    return driver, dict_dlb_elements\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_dlb_elements(driver, dict_dlb_elements, click_count):\n",
    "    print(dict_dlb_elements)\n",
    "    \n",
    "    for key, elements in dict_dlb_elements.items():\n",
    "        print(key)\n",
    "        if not elements:\n",
    "            continue\n",
    "        index_elements = len(elements) - 1\n",
    "        q = 0\n",
    "        while q <= index_elements:\n",
    "            try:\n",
    "                # get and access fresh elements with old key, structure is the same\n",
    "                driver, fresh_dict_dlb_elements = get_dlb_elements(driver)\n",
    "                fresh_elements = fresh_dict_dlb_elements[key]\n",
    "                fresh_elements[q].click()\n",
    "                sleep(round(random.uniform(0.31, 2.11), 2))\n",
    "                # download and go back\n",
    "                driver.find_element(By.ID, \"form:kostenpflichtigabrufen\").click()\n",
    "                click_count += 1\n",
    "                driver.back()\n",
    "                print(\"sucessfully downloaded {0} \\nat q= {1}\".format(elements[q], q))\n",
    "            except Exception as exc:\n",
    "                print('ERROR while downloading {0} \\nat q= {1} \\nexception: {2}'.format(elements[q], q, exc))\n",
    "            q += 1\n",
    "            print(\"q after download: \", q)    \n",
    "    \n",
    "    \n",
    "            \n",
    "    return  driver, click_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(swort, nsitz, ip, port, click_count, select_ip, valid_proxy, flag_proxy=False):\n",
    "    print(swort)\n",
    "    print(nsitz)\n",
    "    \n",
    "    try:\n",
    "        driver = get_driver(flag_proxy=flag_proxy, ip=ip, port=port)\n",
    "        URL = \"https://www.handelsregister.de/rp_web/erweitertesuche.xhtml\"\n",
    "        # URL = \"https://bot.sannysoft.com/\"    # test site for bot detection\n",
    "        # URL = \"https://nowsecure.nl\"          # test site with max anti-bot protection\n",
    "        driver.get(URL)\n",
    "        \n",
    "        # input\n",
    "        input_swort = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.ID, \"form:schlagwoerter\")))\n",
    "        input_nsitz = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.ID, \"form:NiederlassungSitz\")))\n",
    "        input_swort.clear()\n",
    "        input_nsitz.clear()\n",
    "\n",
    "        input_swort.send_keys(swort)\n",
    "        sleep(round(random.uniform(0.31, 2.11), 2))\n",
    "        input_nsitz.send_keys(nsitz)\n",
    "        sleep(round(random.uniform(0.31, 2.11), 2))\n",
    "\n",
    "        # submit does not work, so click button\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(round(random.uniform(0.31, 2.11), 2))\n",
    "        driver.find_element(By.ID, \"form:btnSuche\").click()\n",
    "        sleep(round(random.uniform(0.31, 2.11), 2))\n",
    "\n",
    "        # first get_dlb_elements\n",
    "        driver, dict_dlb_elements = get_dlb_elements(driver)\n",
    "        \n",
    "        # DK_elements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.LINK_TEXT, \"DK\")))\n",
    "        # AD_elements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.LINK_TEXT, \"UT\")))\n",
    "        # AD_elements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.LINK_TEXT, \"VÖ\")))\n",
    "        # AD_elements = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.LINK_TEXT, \"SI\")))\n",
    "        \n",
    "    except Exception as exc:\n",
    "        select_ip +=1\n",
    "        valid_proxy = False\n",
    "        print('ERROR with ip: {0} and port: {1} generated an exception: {2}\\n try next ...'.format(ip, port, exc))\n",
    "        driver.quit()\n",
    "        return click_count, select_ip, valid_proxy\n",
    "    \n",
    "    driver, click_count = scrape_dlb_elements(driver, dict_dlb_elements, click_count)\n",
    "    print(\"reached quit\")\n",
    "    driver.quit()\n",
    "    valid_proxy = True   \n",
    "     \n",
    "    return driver, click_count, select_ip, valid_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example premium proxies\n",
    "list_proxies = [#{'IP Address': '45.94.47.66', 'Port': '8110'},\n",
    "                {'IP Address': '45.155.68.129', 'Port': '8133'},\n",
    "                {'IP Address': '185.199.228.220', 'Port': '7300'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>regricht</th>\n",
       "      <th>regart</th>\n",
       "      <th>regnum</th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Henkel AG &amp; Co. KGaA</td>\n",
       "      <td>Düsseldorf</td>\n",
       "      <td>HRB</td>\n",
       "      <td>4724</td>\n",
       "      <td>https://www.henkel.de/</td>\n",
       "      <td>Consumer Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bau-Service Baur GmbH</td>\n",
       "      <td>Ulm</td>\n",
       "      <td>HRB</td>\n",
       "      <td>490392</td>\n",
       "      <td>https://bauservice-baur.de/</td>\n",
       "      <td>Construction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Schramm Bauschutt-Recycling GmbH &amp; Co.</td>\n",
       "      <td>Coburg</td>\n",
       "      <td>HRB</td>\n",
       "      <td>2320</td>\n",
       "      <td>https://www.bauschuttdeponie-schramm.de/</td>\n",
       "      <td>Renewables &amp; Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>​PHOENIX Pharma SE</td>\n",
       "      <td>Mannheim</td>\n",
       "      <td>HRB</td>\n",
       "      <td>727494</td>\n",
       "      <td>https://www.phoenixgroup.eu/de/</td>\n",
       "      <td>Wholesale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nestlé Deutschland AG</td>\n",
       "      <td>Frankfurt am Main</td>\n",
       "      <td>HRB</td>\n",
       "      <td>28163</td>\n",
       "      <td>https://www.nestle.de/</td>\n",
       "      <td>Consumer Goods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name           regricht regart  regnum  \\\n",
       "13                     Henkel AG & Co. KGaA         Düsseldorf    HRB    4724   \n",
       "7                     Bau-Service Baur GmbH                Ulm    HRB  490392   \n",
       "99   Schramm Bauschutt-Recycling GmbH & Co.            Coburg     HRB    2320   \n",
       "108                      ​PHOENIX Pharma SE          Mannheim     HRB  727494   \n",
       "17                    Nestlé Deutschland AG  Frankfurt am Main    HRB   28163   \n",
       "\n",
       "                                          url                     label  \n",
       "13                     https://www.henkel.de/            Consumer Goods  \n",
       "7                 https://bauservice-baur.de/              Construction  \n",
       "99   https://www.bauschuttdeponie-schramm.de/  Renewables & Environment  \n",
       "108           https://www.phoenixgroup.eu/de/                 Wholesale  \n",
       "17                     https://www.nestle.de/            Consumer Goods  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Einlesen der gelabelten URLs, samt Registerinformationen zur Suche im Handelsregister\n",
    "df_terms = pd.read_excel(\"C:\\\\Users\\\\lange\\\\OneDrive\\\\Dokumente\\\\Studium\\\\06_Abschlussarbeiten\\\\Seminar\\\\UN_HRB.xlsx\")\n",
    "df_terms.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>swort</th>\n",
       "      <th>nsitz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTENSE AG</td>\n",
       "      <td>Würzburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snapADDY GmbH</td>\n",
       "      <td>Würzburg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           swort     nsitz\n",
       "0     INTENSE AG  Würzburg\n",
       "1  snapADDY GmbH  Würzburg"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example df\n",
    "data = [['INTENSE AG', 'Würzburg'], ['snapADDY GmbH', 'Würzburg']]\n",
    "df_terms = pd.DataFrame(data, columns=['swort', 'nsitz'])\n",
    "df_terms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calls\n",
    "\n",
    "Läuft mit funktionierenden Proxies allerdings sind die Proxies aus dem Beispiel inzwischen wieder veraltet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTENSE AG\n",
      "Würzburg\n",
      "45.155.68.129:8133\n",
      "ERROR with ip: 45.155.68.129 and port: 8133 generated an exception: Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\t(No symbol) [0x0072F243]\n",
      "\t(No symbol) [0x006B7FD1]\n",
      "\t(No symbol) [0x005AD04D]\n",
      "\t(No symbol) [0x005DC0B0]\n",
      "\t(No symbol) [0x005DC22B]\n",
      "\t(No symbol) [0x0060E612]\n",
      "\t(No symbol) [0x005F85D4]\n",
      "\t(No symbol) [0x0060C9EB]\n",
      "\t(No symbol) [0x005F8386]\n",
      "\t(No symbol) [0x005D163C]\n",
      "\t(No symbol) [0x005D269D]\n",
      "\tGetHandleVerifier [0x009C9A22+2655074]\n",
      "\tGetHandleVerifier [0x009BCA24+2601828]\n",
      "\tGetHandleVerifier [0x007D8C0A+619850]\n",
      "\tGetHandleVerifier [0x007D7830+614768]\n",
      "\t(No symbol) [0x006C05FC]\n",
      "\t(No symbol) [0x006C5968]\n",
      "\t(No symbol) [0x006C5A55]\n",
      "\t(No symbol) [0x006D051B]\n",
      "\tBaseThreadInitThunk [0x75AD6BD9+25]\n",
      "\tRtlGetFullPathName_UEx [0x77108FD2+1218]\n",
      "\tRtlGetFullPathName_UEx [0x77108F9D+1165]\n",
      "\n",
      " try next ...\n",
      "ERROR not enough values to unpack (expected 4, got 3)\n",
      "INTENSE AG\n",
      "Würzburg\n",
      "45.155.68.129:8133\n",
      "ERROR with ip: 45.155.68.129 and port: 8133 generated an exception: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=109.0.5414.46)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\t(No symbol) [0x0070F243]\n",
      "\t(No symbol) [0x00697FD1]\n",
      "\t(No symbol) [0x0058D04D]\n",
      "\t(No symbol) [0x00572D7A]\n",
      "\t(No symbol) [0x005DBE7B]\n",
      "\t(No symbol) [0x005EC196]\n",
      "\t(No symbol) [0x005D8386]\n",
      "\t(No symbol) [0x005B163C]\n",
      "\t(No symbol) [0x005B269D]\n",
      "\tGetHandleVerifier [0x009A9A22+2655074]\n",
      "\tGetHandleVerifier [0x0099CA24+2601828]\n",
      "\tGetHandleVerifier [0x007B8C0A+619850]\n",
      "\tGetHandleVerifier [0x007B7830+614768]\n",
      "\t(No symbol) [0x006A05FC]\n",
      "\t(No symbol) [0x006A5968]\n",
      "\t(No symbol) [0x006A5A55]\n",
      "\t(No symbol) [0x006B051B]\n",
      "\tBaseThreadInitThunk [0x75AD6BD9+25]\n",
      "\tRtlGetFullPathName_UEx [0x77108FD2+1218]\n",
      "\tRtlGetFullPathName_UEx [0x77108F9D+1165]\n",
      "\n",
      " try next ...\n",
      "ERROR not enough values to unpack (expected 4, got 3)\n",
      "INTENSE AG\n",
      "Würzburg\n",
      "45.155.68.129:8133\n",
      "ERROR with ip: 45.155.68.129 and port: 8133 generated an exception: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=109.0.5414.46)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\t(No symbol) [0x004BF243]\n",
      "\t(No symbol) [0x00447FD1]\n",
      "\t(No symbol) [0x0033D04D]\n",
      "\t(No symbol) [0x00322D7A]\n",
      "\t(No symbol) [0x0038BE7B]\n",
      "\t(No symbol) [0x0039C196]\n",
      "\t(No symbol) [0x00388386]\n",
      "\t(No symbol) [0x0036163C]\n",
      "\t(No symbol) [0x0036269D]\n",
      "\tGetHandleVerifier [0x00759A22+2655074]\n",
      "\tGetHandleVerifier [0x0074CA24+2601828]\n",
      "\tGetHandleVerifier [0x00568C0A+619850]\n",
      "\tGetHandleVerifier [0x00567830+614768]\n",
      "\t(No symbol) [0x004505FC]\n",
      "\t(No symbol) [0x00455968]\n",
      "\t(No symbol) [0x00455A55]\n",
      "\t(No symbol) [0x0046051B]\n",
      "\tBaseThreadInitThunk [0x75AD6BD9+25]\n",
      "\tRtlGetFullPathName_UEx [0x77108FD2+1218]\n",
      "\tRtlGetFullPathName_UEx [0x77108F9D+1165]\n",
      "\n",
      " try next ...\n",
      "ERROR not enough values to unpack (expected 4, got 3)\n",
      "INTENSE AG\n",
      "Würzburg\n",
      "45.155.68.129:8133\n"
     ]
    }
   ],
   "source": [
    "# call scraper\n",
    "# check different methods for faster applying functions to data\n",
    "# https://www.ml4devs.com/articles/pandas-dataframe-apply-function-iterate-over-rows/\n",
    "\n",
    "select_ip = 0\n",
    "click_count = 0\n",
    "for index, row in df_terms.iterrows():\n",
    "    \n",
    "    valid_proxy = False\n",
    "    while valid_proxy == False:\n",
    "        \n",
    "        if click_count >= 55:\n",
    "            select_ip +=1\n",
    "            click_count = 0\n",
    "    \n",
    "        dict_proxy = list_proxies[select_ip]\n",
    "        ip = dict_proxy[\"IP Address\"]\n",
    "        port = dict_proxy[\"Port\"]\n",
    "    \n",
    "        try:\n",
    "            driver, click_count, select_ip, valid_proxy = scrape(swort=row['swort'], \n",
    "                                                                nsitz=row['nsitz'], \n",
    "                                                                flag_proxy=True, \n",
    "                                                                ip=ip, \n",
    "                                                                port=port, \n",
    "                                                                click_count=click_count, \n",
    "                                                                select_ip=select_ip, \n",
    "                                                                valid_proxy=valid_proxy)\n",
    "        except Exception as exc:\n",
    "            print(\"ERROR {}\".format(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI Threading\n",
    "id, swort, nsitz = get_terms(terms=terms)\n",
    "# default number of threads is optimized for cpu cores \n",
    "# possible to set with `max_workers` like `futures.ThreadPoolExecutor(max_workers=...)`\n",
    "with futures.ThreadPoolExecutor() as executor:     \n",
    "    # store the id for each thread as a dict, so we can know which thread fails\n",
    "    future_results = { id : executor.submit(scrape(swort, nsitz), terms) for id in terms }\n",
    "    for id, future in future_results.items(): \n",
    "        try:        \n",
    "           future.result() # can use `timeout` to wait max seconds for each thread  \n",
    "        except Exception as exc: # can give a exception in some thread\n",
    "           print('id {0} generated an exception: {1}'.format(id, exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROXY TEST\n",
    "PROXY = \"174.138.16.96:8888\" # IP:PORT or HOST:PORT\n",
    "\n",
    "chrome_options = uc.ChromeOptions()\n",
    "chrome_options.add_argument('--proxy-server=http://%s' % PROXY)\n",
    "\n",
    "driver = uc.Chrome(options=chrome_options)\n",
    "driver.get(\"http://whatismyipaddress.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('handelreg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ecc71f0c805e7b0299b362458c917069b40f45b7a664663fd15581e0a2e2c8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
